{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b8fee179-4bf6-4d85-b1c5-6071d2056ddf",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "iks4T4DPy-ya"
      },
      "outputs": [],
      "source": [
        "# --- 1. Kütüphane Kurulumu ---\n",
        "# TensorFlow ve Horovod'u kuruyoruz. Databricks kısıtlamalarını aşmak için --upgrade kullanıyoruz.\n",
        "%pip install tensorflow --upgrade\n",
        "%pip install horovod --upgrade\n",
        "\n",
        "# Kütüphane kurulumundan sonra Python'ı yeniden başlatma (Zorunludur)\n",
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e328325e-280a-4d05-8c8f-970184af9c74",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Zx7iKo3My-yd"
      },
      "outputs": [],
      "source": [
        "# --- 2. Dağıtık Eğitim Kodu ---\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import horovod.tensorflow.keras as hvd\n",
        "from horovod.spark import run # Horovod Runner için kritik\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import rand, udf, col\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# Spark Oturumunu Başlatma\n",
        "spark = SparkSession.builder.appName(\"HorovodMinimalExample\").getOrCreate()\n",
        "print(f\"Spark Uygulama Adı: {spark.sparkContext.appName}\")\n",
        "print(f\"TensorFlow Sürümü: {tf.__version__}\")\n",
        "print(f\"Horovod Sürümü: {hvd.__version__}\")\n",
        "\n",
        "# --- 1. Veri Hazırlama (Spark DataFrame) ---\n",
        "\n",
        "# Horovod'un Spark Runner'ı PySpark VectorDense formatında veri bekler.\n",
        "def create_minimal_data(num_rows=10000):\n",
        "    # Sentetik veri oluşturma: Tek özellik\n",
        "    df = spark.range(num_rows).withColumn(\"feature_raw\", rand(seed=42))\n",
        "\n",
        "    # PySpark Vector ve Label formatına dönüştürme\n",
        "    vector_udf = udf(lambda x: Vectors.dense([float(x)]), VectorUDT())\n",
        "\n",
        "    @udf(returnType=FloatType())\n",
        "    def create_label(feature_val):\n",
        "        # Basit sınıflandırma: feature > 0.5 ise 1, değilse 0\n",
        "        return float(feature_val > 0.5)\n",
        "\n",
        "    processed_df = df.withColumn(\"features\", vector_udf(col(\"feature_raw\")))\\\n",
        "                     .withColumn(\"label\", create_label(col(\"feature_raw\")))\\\n",
        "                     .select(\"features\", \"label\")\n",
        "\n",
        "    # Veriyi Spark çekirdeklerine dağıtma (Shuffle)\n",
        "    return processed_df.repartition(4) # Örnek olarak 4 bölüme ayırıyoruz\n",
        "\n",
        "train_df = create_minimal_data()\n",
        "print(\"\\nSpark DataFrame Hazır.\")\n",
        "train_df.printSchema()\n",
        "\n",
        "# --- 2. Dağıtık Çalıştırılacak Eğitim Fonksiyonunun Tanımlanması ---\n",
        "\n",
        "def minimal_train_fn(learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Horovod Runner tarafından her Spark Executor'da çalıştırılacak fonksiyon.\n",
        "    \"\"\"\n",
        "    # 1. Horovod'u Başlatma (Her Worker'da Çalışır)\n",
        "    hvd.init()\n",
        "\n",
        "    # GPU/CPU Yapılandırması (GPU'lu küme varsayılırsa)\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        # Her worker'a farklı bir GPU atama\n",
        "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
        "        tf.config.experimental.set_memory_growth(gpus[hvd.local_rank()], True)\n",
        "\n",
        "    # 2. Model Tanımlama\n",
        "    input_dim = 1\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(32, activation='relu', input_shape=(input_dim,)),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # 3. Dağıtık Optimizatör\n",
        "    # Öğrenme hızı ölçeklendirilir\n",
        "    opt = keras.optimizers.Adam(learning_rate=learning_rate * hvd.size())\n",
        "    opt = hvd.DistributedOptimizer(opt) # Gradien toplama (Allreduce)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 4. Geri Çağrımlar (Callbacks)\n",
        "    callbacks = [\n",
        "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
        "        hvd.callbacks.MetricAverageCallback(),\n",
        "    ]\n",
        "\n",
        "    if hvd.rank() == 0:\n",
        "        print(f\"Toplam Horovod Worker sayısı: {hvd.size()}\")\n",
        "\n",
        "    # Modeli ve geri çağrımları döndürme\n",
        "    return model, callbacks\n",
        "\n",
        "# --- 3. Horovod Runner ile Eğitimi Başlatma ---\n",
        "\n",
        "num_processes = 4 # 4 Spark çekirdeği/worker kullan\n",
        "print(f\"\\n--- Horovod Spark Runner {num_processes} Worker ile Başlatılıyor ---\")\n",
        "\n",
        "spark_model = run(\n",
        "    minimal_train_fn,\n",
        "    num_proc=num_processes,\n",
        "    args=(0.001,), # train_fn'e öğrenme hızı parametresini gönderme\n",
        "    data_frame=train_df,\n",
        "    verbose=1,\n",
        "    epochs=5,\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "print(\"\\n--- Dağıtık Eğitim Tamamlandı ---\")\n",
        "\n",
        "# Eğitilmiş modeli kullanarak tahmin yapma\n",
        "if spark_model:\n",
        "    prediction_df = spark_model.transform(train_df)\n",
        "    prediction_df.select(\"label\", \"prediction\").show(5)\n",
        "\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "4"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "10_12_2025_Notebook",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}