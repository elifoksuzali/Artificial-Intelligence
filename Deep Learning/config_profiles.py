"""
NAT Model KonfigÃ¼rasyon Profilleri
==================================

FarklÄ± donanÄ±m ortamlarÄ± iÃ§in optimize edilmiÅŸ ayarlar.
"""

import torch
from dataclasses import dataclass
from pathlib import Path


@dataclass
class ConfigProfile:
    """KonfigÃ¼rasyon profili"""
    name: str
    batch_size: int
    img_size: int
    num_workers: int
    use_amp: bool  # Mixed precision
    gradient_accumulation: int
    model_variant: str  # 'mini', 'tiny', 'small', 'base'
    
    # NAT model parametreleri
    embed_dim: int
    depths: tuple
    num_heads: tuple
    kernel_size: int


# =============================================================================
# DONANIM PROFÄ°LLERÄ°
# =============================================================================

# ğŸŸ¢ DÃœÅÃœK VRAM (2-4 GB) - GTX 1650, MX450, vb.
LOW_VRAM = ConfigProfile(
    name="low_vram",
    batch_size=4,
    img_size=224,
    num_workers=2,
    use_amp=True,  # Bellek tasarrufu iÃ§in
    gradient_accumulation=4,  # Effective batch size = 16
    model_variant='mini',
    embed_dim=48,  # KÃ¼Ã§Ã¼ltÃ¼lmÃ¼ÅŸ
    depths=(2, 2, 4, 2),
    num_heads=(2, 4, 6, 12),
    kernel_size=5,  # KÃ¼Ã§Ã¼k kernel
)

# ğŸŸ¡ ORTA VRAM (4-8 GB) - GTX 1660, RTX 2060, vb.
MEDIUM_VRAM = ConfigProfile(
    name="medium_vram",
    batch_size=8,
    img_size=224,
    num_workers=4,
    use_amp=True,
    gradient_accumulation=2,  # Effective batch size = 16
    model_variant='mini',
    embed_dim=64,
    depths=(2, 2, 6, 2),
    num_heads=(2, 4, 8, 16),
    kernel_size=7,
)

# ğŸŸ¢ YÃœKSEK VRAM (8-12 GB) - RTX 3060, RTX 3070, vb.
HIGH_VRAM = ConfigProfile(
    name="high_vram",
    batch_size=16,
    img_size=224,
    num_workers=4,
    use_amp=True,
    gradient_accumulation=1,
    model_variant='tiny',
    embed_dim=64,
    depths=(3, 4, 6, 5),
    num_heads=(2, 4, 8, 16),
    kernel_size=7,
)

# ğŸ”µ COLAB / KAGGLE (15-16 GB)
COLAB = ConfigProfile(
    name="colab",
    batch_size=24,
    img_size=224,
    num_workers=2,  # Colab'da dÃ¼ÅŸÃ¼k tut
    use_amp=True,
    gradient_accumulation=1,
    model_variant='tiny',
    embed_dim=64,
    depths=(3, 4, 6, 5),
    num_heads=(2, 4, 8, 16),
    kernel_size=7,
)

# ğŸ”´ CPU ONLY (GPU yok)
CPU_ONLY = ConfigProfile(
    name="cpu_only",
    batch_size=4,
    img_size=192,  # KÃ¼Ã§Ã¼k gÃ¶rÃ¼ntÃ¼
    num_workers=0,  # Windows'ta multiprocessing sorunu Ã¶nlemek iÃ§in
    use_amp=False,  # CPU'da AMP yok
    gradient_accumulation=4,
    model_variant='mini',
    embed_dim=32,  # Ã‡ok kÃ¼Ã§Ã¼k model
    depths=(1, 1, 2, 1),
    num_heads=(1, 2, 4, 8),
    kernel_size=5,
)


def get_profile_for_system() -> ConfigProfile:
    """
    Sistem donanÄ±mÄ±na gÃ¶re otomatik profil seÃ§
    """
    if not torch.cuda.is_available():
        print("âš ï¸ GPU bulunamadÄ±, CPU profili kullanÄ±lÄ±yor")
        return CPU_ONLY
    
    # GPU belleÄŸini kontrol et
    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    gpu_name = torch.cuda.get_device_name(0)
    
    print(f"ğŸ–¥ï¸ GPU: {gpu_name}")
    print(f"ğŸ’¾ VRAM: {gpu_memory_gb:.1f} GB")
    
    if gpu_memory_gb < 4:
        print("ğŸ“‹ Profil: LOW_VRAM")
        return LOW_VRAM
    elif gpu_memory_gb < 8:
        print("ğŸ“‹ Profil: MEDIUM_VRAM")
        return MEDIUM_VRAM
    elif gpu_memory_gb < 14:
        print("ğŸ“‹ Profil: HIGH_VRAM")
        return HIGH_VRAM
    else:
        print("ğŸ“‹ Profil: COLAB/HIGH-END")
        return COLAB


def print_profile_info(profile: ConfigProfile):
    """Profil bilgilerini yazdÄ±r"""
    print("\n" + "=" * 50)
    print(f"ğŸ“‹ KonfigÃ¼rasyon Profili: {profile.name.upper()}")
    print("=" * 50)
    print(f"  Batch Size: {profile.batch_size}")
    print(f"  Image Size: {profile.img_size}x{profile.img_size}")
    print(f"  Gradient Accumulation: {profile.gradient_accumulation}")
    print(f"  Effective Batch Size: {profile.batch_size * profile.gradient_accumulation}")
    print(f"  Mixed Precision (AMP): {profile.use_amp}")
    print(f"  Model Variant: {profile.model_variant}")
    print(f"  Embed Dim: {profile.embed_dim}")
    print(f"  Depths: {profile.depths}")
    print(f"  Num Heads: {profile.num_heads}")
    print(f"  Kernel Size: {profile.kernel_size}")
    print("=" * 50)


def estimate_memory_usage(profile: ConfigProfile) -> dict:
    """
    Tahmini bellek kullanÄ±mÄ±nÄ± hesapla
    """
    # YaklaÅŸÄ±k hesaplama
    img_size = profile.img_size
    batch_size = profile.batch_size
    embed_dim = profile.embed_dim
    
    # Model parametreleri (yaklaÅŸÄ±k)
    total_depth = sum(profile.depths)
    params = embed_dim * embed_dim * total_depth * 12  # YaklaÅŸÄ±k
    
    # Aktivasyonlar (forward pass)
    activation_size = batch_size * (img_size // 4) ** 2 * embed_dim * 4
    
    # Gradyanlar (backward pass)
    gradient_size = params * 4  # float32
    
    # Toplam (byte)
    total_bytes = params * 4 + activation_size + gradient_size
    
    # AMP ile %30-40 tasarruf
    if profile.use_amp:
        total_bytes *= 0.65
    
    return {
        'model_params_mb': params * 4 / (1024**2),
        'activations_mb': activation_size / (1024**2),
        'gradients_mb': gradient_size / (1024**2),
        'total_estimated_gb': total_bytes / (1024**3),
    }


# =============================================================================
# TEST
# =============================================================================

if __name__ == "__main__":
    print("\nğŸ” Sistem Analizi")
    print("-" * 50)
    
    # Otomatik profil seÃ§
    profile = get_profile_for_system()
    print_profile_info(profile)
    
    # Bellek tahmini
    memory = estimate_memory_usage(profile)
    print(f"\nğŸ’¾ Tahmini Bellek KullanÄ±mÄ±:")
    print(f"  Model: {memory['model_params_mb']:.1f} MB")
    print(f"  Aktivasyonlar: {memory['activations_mb']:.1f} MB")
    print(f"  Gradyanlar: {memory['gradients_mb']:.1f} MB")
    print(f"  Toplam: ~{memory['total_estimated_gb']:.2f} GB")
    
    # TÃ¼m profilleri listele
    print("\n" + "=" * 50)
    print("ğŸ“‹ TÃœM PROFÄ°LLER")
    print("=" * 50)
    
    profiles = [CPU_ONLY, LOW_VRAM, MEDIUM_VRAM, HIGH_VRAM, COLAB]
    
    print(f"{'Profil':<15} {'Batch':<8} {'Img':<8} {'AMP':<6} {'VRAM Gereksinimi':<20}")
    print("-" * 60)
    
    for p in profiles:
        mem = estimate_memory_usage(p)
        print(f"{p.name:<15} {p.batch_size:<8} {p.img_size:<8} {str(p.use_amp):<6} ~{mem['total_estimated_gb']:.1f} GB")

