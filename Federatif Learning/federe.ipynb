{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"federe.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMIQOFZCC8JZLqua07NhzNn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import pandas as pd\n","import random\n","import argparse\n","import requests\n","import zipfile\n","from tqdm import tqdm"],"metadata":{"id":"u_XSBIKVHjQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser(description='Run Data Preprocessor.')\n","\n","parser.add_argument(\n","    '--turbofan_dataset_id',\n","    type=str,\n","    help='ID of the turbofan dataset, e.g. FD004.',\n","    default=os.environ.get('TURBOFAN_DATASET_ID', 'FD004'),\n",")\n","parser.add_argument(\n","    '--engine_percentage_initial',\n","    type=int,\n","    help='Percentage of train engines used for initial model training.',\n","    default=os.environ.get('ENGINE_PERCENTAGE_INITIAL', 10),\n",")\n","\n","parser.add_argument(\n","    '--engine_percentage_val',\n","    type=int,\n","    help='Percentage of test engines used for cross validation.',\n","    default=os.environ.get('ENGINE_PERCENTAGE_VAL', 50),\n",")\n","\n","parser.add_argument(\n","    '--worker_count',\n","    type=int,\n","    help='Number of workers used.',\n","    default=os.environ.get('WORKER_COUNT', 6),\n",")\n","\n","parser.add_argument(\n","    '--no_download',\n","    help='Dont Download datasets when argument is present.',\n","    action='store_true',\n",")\n","\n","\n","def download_datasets():\n","    \"\"\" Download and unzip the NASA turbofan dataset. \"\"\"\n","    file_name = \"data.zip\"\n","    url = \"http://ti.arc.nasa.gov/c/6/\"\n","    response = requests.get(url, stream=True)\n","    with open(file_name, \"wb\") as handle:\n","        for data in tqdm(response.iter_content()):\n","            handle.write(data)\n","\n","    my_zip = zipfile.ZipFile('data.zip')\n","    storage_path = './data/'\n","    for file in my_zip.namelist():\n","        if my_zip.getinfo(file).filename.endswith('.txt'):\n","            my_zip.extract(file, storage_path)\n","\n","    os.remove(file_name)\n","\n","\n","def import_data(dataset_id):\n","    \"\"\" Import the turbofan training and test data and the test RUL values from the data files.\n","    :param dataset_id: The dataset from turbofan to import\n","    :return: A tuple with the training dataset, the test dataset and the test rul data\n","    \"\"\"\n","    # define the columns in the dataset\n","    operational_settings = ['operational_setting_{}'.format(i + 1) for i in range(3)]\n","    sensor_columns = ['sensor_measurement_{}'.format(i + 1) for i in range(23)]\n","    cols = ['engine_no', 'time_in_cycles'] + operational_settings + sensor_columns\n","\n","    # load the data\n","    dirname = os.getcwd()\n","    folder_path = os.path.join(dirname, 'data')\n","\n","    train_path = os.path.join(folder_path, 'train_{}.txt'.format(dataset_id))\n","    train_data = pd.read_csv(train_path, delim_whitespace=True, header=None, names=cols)\n","    train_data.set_index('time_in_cycles')\n","    test_path = os.path.join(folder_path, 'test_{}.txt'.format(dataset_id))\n","    test_data = pd.read_csv(test_path, delim_whitespace=True, header=None, names=cols)\n","    test_data.set_index('time_in_cycles')\n","    test_data_rul_path = os.path.join(folder_path, 'RUL_{}.txt'.format(dataset_id))\n","    test_data_rul = pd.read_csv(test_data_rul_path, delim_whitespace=True, header=None, names=['RUL'])\n","\n","    return train_data, test_data, test_data_rul\n","\n","\n","def save_data(train_data_initial, train_data_worker, test_data_val, test_data_test):\n","    \"\"\" Save the prepared data sets into csv files.\n","    :param train_data_initial: The data for initial training to save\n","    :param train_data_worker: An array of data for every worker to save\n","    :param test_data_val: The validation data to save\n","    :param test_data_test: The test data to save\n","    :return: None\n","    \"\"\"\n","    dirname = os.getcwd()\n","    folder_path = os.path.join(dirname, 'data')\n","\n","    train_data_initial_path = os.path.join(folder_path, 'train_data_initial.txt')\n","    train_data_initial.to_csv(train_data_initial_path, index=False)\n","\n","    for index, data in enumerate(train_data_worker):\n","        train_data_worker_path = os.path.join(folder_path, 'train_data_worker_{}.txt'.format(index + 1))\n","        data.to_csv(train_data_worker_path, index=False)\n","\n","    test_data_val_path = os.path.join(folder_path, 'test_data_val.txt')\n","    test_data_val.to_csv(test_data_val_path, index=False)\n","\n","    test_data_test_path = os.path.join(folder_path, 'test_data_test.txt')\n","    test_data_test.to_csv(test_data_test_path, index=False)\n","\n","\n","def add_rul_to_test_data(test_data, test_data_rul):\n","    \"\"\" Enhance each row in the test data with the RUL. This is done inplace.\n","    :param test_data: The test data to enhance\n","    :param test_data_rul: The final RUL values for the engines in the test data\n","    :return: None\n","    \"\"\"\n","    # prepare the RUL file data\n","    test_data_rul['engine_no'] = test_data_rul.index + 1\n","    test_data_rul.columns = ['final_rul', 'engine_no']\n","\n","    # retrieve the max cycles in the test data\n","    test_rul_max = pd.DataFrame(test_data.groupby('engine_no')['time_in_cycles'].max()).reset_index()\n","    test_rul_max.columns = ['engine_no', 'max']\n","\n","    test_data = test_data.merge(test_data_rul, on=['engine_no'], how='left')\n","    test_data = test_data.merge(test_rul_max, on=['engine_no'], how='left')\n","\n","    # add the current RUL for every cycle\n","    test_data['RUL'] = test_data['max'] + test_data['final_rul'] - test_data['time_in_cycles']\n","    test_data.drop(['max', 'final_rul'], axis=1, inplace=True)\n","\n","    return test_data\n","\n","\n","def split_train_data_by_engines(train_data, engine_percentage_initial, worker_count):\n","    \"\"\" Groups the train data by engines and split it into subsets for initial training and for each worker.\n","    :param train_data: The full training data set\n","    :param engine_percentage_initial: The percentage of engines to take for initial training\n","    :param worker_count: The number of workers to prepare data sets for\n","    :return: A tuple with the initial training data and an array of the worker data\n","    \"\"\"\n","    train_data_per_engines = train_data.groupby('engine_no')\n","    train_data_per_engines = [train_data_per_engines.get_group(x) for x in train_data_per_engines.groups]\n","    random.shuffle(train_data_per_engines)\n","\n","    # split into data for initial training and data for the worker nodes\n","    engine_count_initial = int(len(train_data_per_engines) * engine_percentage_initial / 100)\n","    train_data_initial = pd.concat(train_data_per_engines[:engine_count_initial])\n","    train_data_worker_all = train_data_per_engines[engine_count_initial:]\n","\n","    train_data_worker = []\n","    engine_count_worker = int((len(train_data_per_engines) - engine_count_initial) / worker_count)\n","\n","    # split worker data into the data sets for every single worker\n","    for i in range(worker_count):\n","        start = i * engine_count_worker\n","        end = start + engine_count_worker\n","        train_data_worker.append(pd.concat(train_data_worker_all[start:end]))\n","\n","    return train_data_initial, train_data_worker\n","\n","\n","def split_test_data_by_engines(test_data, engine_percentage_val):\n","    \"\"\" Groups the train data by engines and split it into a subset for validation and one for testing.\n","    :param test_data: The full test data set\n","    :param engine_percentage_val: The percentage of engines to take for validation\n","    :return: A tuple of the validation and the test data\n","    \"\"\"\n","    test_data_per_engines = test_data.groupby('engine_no')\n","    test_data_per_engines = [test_data_per_engines.get_group(x) for x in test_data_per_engines.groups]\n","    random.shuffle(test_data_per_engines)\n","\n","    engine_count_val = int(len(test_data_per_engines) * engine_percentage_val / 100)\n","    test_data_val = pd.concat(test_data_per_engines[:engine_count_val])\n","    test_data_test = pd.concat(test_data_per_engines[engine_count_val:])\n","\n","    return test_data_val, test_data_test\n","\n","\n","if __name__ == \"__main__\":\n","    args = parser.parse_args()\n","\n","    # read in the arguments\n","    dataset_id = args.turbofan_dataset_id\n","    engine_percentage_initial = args.engine_percentage_initial\n","    engine_percentage_val = args.engine_percentage_val\n","    worker_count = args.worker_count\n","    no_download = args.no_download\n","\n","    if not no_download:\n","        print('Starting download of datasets')\n","        download_datasets()\n","\n","    print(\"\\n##########\")\n","    print(\"Importing data for data set {}\".format(dataset_id))\n","    train_data, test_data, test_data_rul = import_data(dataset_id)\n","    test_data = add_rul_to_test_data(test_data, test_data_rul)\n","\n","    print(\"Splitting training data into subsets\")\n","    print(\"Using {}% data for initial training\".format(engine_percentage_initial))\n","    print(\"Creating subsets for {} worker\".format(worker_count))\n","    train_data_initial, train_data_worker = split_train_data_by_engines(\n","        train_data,\n","        engine_percentage_initial,\n","        worker_count\n","    )\n","    print(\"Splitting test data into sets for validation and testing\")\n","    print(\"Using {}% data for validation\".format(engine_percentage_val))\n","    test_data_val, test_data_test = split_test_data_by_engines(test_data, engine_percentage_val)\n","\n","    print(\"Saving data sets\")\n","    save_data(train_data_initial, train_data_worker, test_data_val, test_data_test)\n","    print(\"Done\")\n","    print(\"##########\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"l6zRf1b1Hl3k","executionInfo":{"status":"error","timestamp":1646519102180,"user_tz":-180,"elapsed":828,"user":{"displayName":"elif oksuzali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14728913008743787906"}},"outputId":"61b13ec7-fb01-42d8-8596-04d623ef4f3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["usage: ipykernel_launcher.py [-h] [--turbofan_dataset_id TURBOFAN_DATASET_ID]\n","                             [--engine_percentage_initial ENGINE_PERCENTAGE_INITIAL]\n","                             [--engine_percentage_val ENGINE_PERCENTAGE_VAL]\n","                             [--worker_count WORKER_COUNT] [--no_download]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-0ba882f6-fed6-4e70-8a40-8ec78a2de878.json\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]}]}