{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤ Common Voice Veri Seti - Apache Spark ile Preprocessing ve EÄŸitim\n",
    "\n",
    "Bu notebook, Mozilla Common Voice veri setini indirip, Apache Spark ile preprocessing yaparak model eÄŸitimi iÃ§in hazÄ±rlar.\n",
    "\n",
    "## Ä°Ã§indekiler:\n",
    "1. KÃ¼tÃ¼phanelerin Kurulumu\n",
    "2. Veri Setinin Ä°ndirilmesi\n",
    "3. Spark Session OluÅŸturma\n",
    "4. Veri YÃ¼kleme ve KeÅŸif\n",
    "5. Preprocessing Pipeline\n",
    "6. Ã–znitelik Ã‡Ä±karma (Feature Extraction)\n",
    "7. Model EÄŸitimi\n",
    "8. DeÄŸerlendirme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ“¦ KÃ¼tÃ¼phanelerin Kurulumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle\n",
    "!pip install pyspark\n",
    "!pip install kaggle\n",
    "!pip install librosa\n",
    "!pip install soundfile\n",
    "!pip install numpy pandas matplotlib seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KÃ¼tÃ¼phaneleri import et\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Spark kÃ¼tÃ¼phaneleri\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, udf, split, explode, lower, trim,\n",
    "    regexp_replace, length, count, avg, max, min, sum,\n",
    "    array, struct, concat, concat_ws\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    FloatType, ArrayType, DoubleType\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, VectorAssembler, StandardScaler,\n",
    "    MinMaxScaler, PCA, OneHotEncoder\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, RandomForestClassifier,\n",
    "    GBTClassifier, MultilayerPerceptronClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import (\n",
    "    MulticlassClassificationEvaluator,\n",
    "    BinaryClassificationEvaluator\n",
    ")\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Ses iÅŸleme\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "print(\"âœ… TÃ¼m kÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ğŸ“¥ Common Voice Veri Setinin Ä°ndirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KAGGLE API AYARLARI\n",
    "# =============================================================================\n",
    "# Kaggle API anahtarÄ±nÄ±zÄ± ayarlayÄ±n\n",
    "# kaggle.json dosyanÄ±zÄ± ~/.kaggle/ dizinine koyun veya aÅŸaÄŸÄ±daki kodu kullanÄ±n\n",
    "\n",
    "# Google Colab iÃ§in:\n",
    "# from google.colab import files\n",
    "# files.upload()  # kaggle.json dosyasÄ±nÄ± yÃ¼kleyin\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "DATASET_NAME = \"mozillaorg/common-voice\"\n",
    "DATA_DIR = \"/content/common-voice\"\n",
    "\n",
    "# Windows iÃ§in alternatif path\n",
    "# DATA_DIR = \"C:/Users/info/Downloads/Veri Bilimi/common-voice\"\n",
    "\n",
    "print(f\"ğŸ“¥ {DATASET_NAME} indiriliyor...\")\n",
    "print(\"â³ Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...\")\n",
    "\n",
    "# Dizini oluÅŸtur\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Veri setini indir\n",
    "!kaggle datasets download -d {DATASET_NAME} -p {DATA_DIR} --unzip\n",
    "\n",
    "print(f\"\\nâœ… Veri indirildi: {DATA_DIR}\")\n",
    "!ls -la {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ä°ndirilen dosyalarÄ± kontrol et\n",
    "import glob\n",
    "\n",
    "def explore_directory(path):\n",
    "    \"\"\"Dizin yapÄ±sÄ±nÄ± keÅŸfet\"\"\"\n",
    "    print(f\"\\nğŸ“‚ Dizin: {path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        level = root.replace(path, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}ğŸ“ {os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files[:10]:  # Ä°lk 10 dosyayÄ± gÃ¶ster\n",
    "            print(f\"{subindent}ğŸ“„ {file}\")\n",
    "        if len(files) > 10:\n",
    "            print(f\"{subindent}... ve {len(files) - 10} dosya daha\")\n",
    "\n",
    "explore_directory(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. âš¡ Spark Session OluÅŸturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPARK SESSION OLUÅTURMA\n",
    "# =============================================================================\n",
    "\n",
    "def create_spark_session(app_name=\"CommonVoicePreprocessing\", memory=\"4g\"):\n",
    "    \"\"\"\n",
    "    Spark session oluÅŸturur\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    app_name : str\n",
    "        Uygulama adÄ±\n",
    "    memory : str\n",
    "        Driver ve executor bellek miktarÄ±\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    SparkSession\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.memory\", memory) \\\n",
    "        .config(\"spark.executor.memory\", memory) \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .config(\"spark.default.parallelism\", \"8\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Log seviyesini ayarla\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "# Spark session baÅŸlat\n",
    "spark = create_spark_session()\n",
    "\n",
    "print(\"âœ… Spark Session oluÅŸturuldu!\")\n",
    "print(f\"ğŸ“Œ Spark Version: {spark.version}\")\n",
    "print(f\"ğŸ“Œ App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"ğŸ“Œ Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ“Š Veri YÃ¼kleme ve KeÅŸif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERÄ° YÃœKLEME\n",
    "# =============================================================================\n",
    "\n",
    "# Common Voice veri seti genellikle TSV formatÄ±nda metadata iÃ§erir\n",
    "# Dosya yapÄ±sÄ±na gÃ¶re yolu gÃ¼ncelleyin\n",
    "\n",
    "def load_metadata(spark, data_dir, file_pattern=\"*.tsv\"):\n",
    "    \"\"\"\n",
    "    Metadata dosyalarÄ±nÄ± yÃ¼kle\n",
    "    \"\"\"\n",
    "    # TSV dosyalarÄ±nÄ± bul\n",
    "    tsv_files = glob.glob(os.path.join(data_dir, \"**\", file_pattern), recursive=True)\n",
    "    \n",
    "    if not tsv_files:\n",
    "        # CSV dene\n",
    "        tsv_files = glob.glob(os.path.join(data_dir, \"**\", \"*.csv\"), recursive=True)\n",
    "    \n",
    "    print(f\"ğŸ“„ Bulunan metadata dosyalarÄ±: {len(tsv_files)}\")\n",
    "    for f in tsv_files[:5]:\n",
    "        print(f\"   - {f}\")\n",
    "    \n",
    "    return tsv_files\n",
    "\n",
    "metadata_files = load_metadata(spark, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# METADATA SPARK DATAFRAME OLARAK YÃœKLE\n",
    "# =============================================================================\n",
    "\n",
    "def load_tsv_to_spark(spark, file_path, delimiter=\"\\t\"):\n",
    "    \"\"\"\n",
    "    TSV/CSV dosyasÄ±nÄ± Spark DataFrame'e yÃ¼kle\n",
    "    \"\"\"\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"delimiter\", delimiter) \\\n",
    "        .option(\"quote\", '\"') \\\n",
    "        .option(\"escape\", '\"') \\\n",
    "        .csv(file_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ä°lk metadata dosyasÄ±nÄ± yÃ¼kle (varsa)\n",
    "if metadata_files:\n",
    "    # TSV iÃ§in tab, CSV iÃ§in virgÃ¼l\n",
    "    delimiter = \"\\t\" if metadata_files[0].endswith(\".tsv\") else \",\"\n",
    "    df_metadata = load_tsv_to_spark(spark, metadata_files[0], delimiter)\n",
    "    \n",
    "    print(\"\\nğŸ“Š Metadata Schema:\")\n",
    "    df_metadata.printSchema()\n",
    "    \n",
    "    print(\"\\nğŸ“Š Ä°lk 5 SatÄ±r:\")\n",
    "    df_metadata.show(5, truncate=False)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Toplam KayÄ±t SayÄ±sÄ±: {df_metadata.count():,}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Metadata dosyasÄ± bulunamadÄ±. LÃ¼tfen DATA_DIR yolunu kontrol edin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERÄ° KALÄ°TESÄ° ANALÄ°ZÄ°\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_data_quality(df):\n",
    "    \"\"\"\n",
    "    Veri kalitesi analizi yapar\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š VERÄ° KALÄ°TESÄ° ANALÄ°ZÄ°\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Toplam kayÄ±t sayÄ±sÄ±\n",
    "    total_rows = df.count()\n",
    "    print(f\"\\nğŸ“Œ Toplam KayÄ±t: {total_rows:,}\")\n",
    "    \n",
    "    # SÃ¼tun sayÄ±sÄ±\n",
    "    print(f\"ğŸ“Œ SÃ¼tun SayÄ±sÄ±: {len(df.columns)}\")\n",
    "    \n",
    "    # Null deÄŸer analizi\n",
    "    print(\"\\nğŸ“Œ Null DeÄŸer Analizi:\")\n",
    "    null_counts = []\n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(col(col_name).isNull()).count()\n",
    "        null_pct = (null_count / total_rows) * 100\n",
    "        null_counts.append((col_name, null_count, null_pct))\n",
    "        if null_count > 0:\n",
    "            print(f\"   - {col_name}: {null_count:,} ({null_pct:.2f}%)\")\n",
    "    \n",
    "    # Tekrar eden kayÄ±tlar\n",
    "    distinct_rows = df.distinct().count()\n",
    "    duplicates = total_rows - distinct_rows\n",
    "    print(f\"\\nğŸ“Œ Tekrar Eden KayÄ±t: {duplicates:,}\")\n",
    "    \n",
    "    return null_counts\n",
    "\n",
    "if 'df_metadata' in dir():\n",
    "    null_analysis = analyze_data_quality(df_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ğŸ”§ Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# METÄ°N TEMÄ°ZLEME FONKSÄ°YONLARI\n",
    "# =============================================================================\n",
    "\n",
    "def clean_text_column(df, column_name, new_column_name=None):\n",
    "    \"\"\"\n",
    "    Metin sÃ¼tununu temizler\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    column_name : str\n",
    "        Temizlenecek sÃ¼tun adÄ±\n",
    "    new_column_name : str, optional\n",
    "        Yeni sÃ¼tun adÄ±\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    if new_column_name is None:\n",
    "        new_column_name = f\"{column_name}_cleaned\"\n",
    "    \n",
    "    df_cleaned = df.withColumn(\n",
    "        new_column_name,\n",
    "        # KÃ¼Ã§Ã¼k harfe Ã§evir\n",
    "        lower(\n",
    "            # Ã–zel karakterleri kaldÄ±r\n",
    "            regexp_replace(\n",
    "                # Fazla boÅŸluklarÄ± kaldÄ±r\n",
    "                regexp_replace(\n",
    "                    # BaÅŸtaki ve sondaki boÅŸluklarÄ± kaldÄ±r\n",
    "                    trim(col(column_name)),\n",
    "                    r\"\\s+\", \" \"\n",
    "                ),\n",
    "                r\"[^a-zA-ZÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄÃœÅÄ°Ã–Ã‡0-9\\s]\", \"\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "print(\"âœ… Metin temizleme fonksiyonlarÄ± hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NULL DEÄER Ä°ÅLEME\n",
    "# =============================================================================\n",
    "\n",
    "def handle_null_values(df, strategy=\"drop\", fill_value=None, subset=None):\n",
    "    \"\"\"\n",
    "    Null deÄŸerleri iÅŸler\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    strategy : str\n",
    "        'drop' - Null iÃ§eren satÄ±rlarÄ± sil\n",
    "        'fill' - Null deÄŸerleri doldur\n",
    "        'mode' - En sÄ±k deÄŸerle doldur\n",
    "        'mean' - Ortalama ile doldur (sayÄ±sal)\n",
    "        'median' - Medyan ile doldur (sayÄ±sal)\n",
    "    fill_value : any\n",
    "        Doldurulacak deÄŸer (strategy='fill' iÃ§in)\n",
    "    subset : list\n",
    "        Ä°ÅŸlenecek sÃ¼tunlar\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    if strategy == \"drop\":\n",
    "        return df.dropna(subset=subset)\n",
    "    \n",
    "    elif strategy == \"fill\":\n",
    "        if fill_value is not None:\n",
    "            return df.fillna(fill_value, subset=subset)\n",
    "        else:\n",
    "            # String iÃ§in boÅŸ string, sayÄ±sal iÃ§in 0\n",
    "            fill_dict = {}\n",
    "            for col_name, dtype in df.dtypes:\n",
    "                if subset is None or col_name in subset:\n",
    "                    if dtype == \"string\":\n",
    "                        fill_dict[col_name] = \"\"\n",
    "                    elif dtype in [\"int\", \"bigint\", \"double\", \"float\"]:\n",
    "                        fill_dict[col_name] = 0\n",
    "            return df.fillna(fill_dict)\n",
    "    \n",
    "    elif strategy == \"mean\":\n",
    "        # SayÄ±sal sÃ¼tunlar iÃ§in ortalama\n",
    "        from pyspark.sql.functions import mean as spark_mean\n",
    "        for col_name, dtype in df.dtypes:\n",
    "            if subset is None or col_name in subset:\n",
    "                if dtype in [\"int\", \"bigint\", \"double\", \"float\"]:\n",
    "                    mean_val = df.select(spark_mean(col(col_name))).collect()[0][0]\n",
    "                    if mean_val is not None:\n",
    "                        df = df.fillna({col_name: mean_val})\n",
    "        return df\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"âœ… Null deÄŸer iÅŸleme fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KATEGORÄ°K VERÄ° DÃ–NÃœÅÃœMÃœ\n",
    "# =============================================================================\n",
    "\n",
    "def encode_categorical_columns(df, columns, encoding_type=\"label\"):\n",
    "    \"\"\"\n",
    "    Kategorik sÃ¼tunlarÄ± encode eder\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    columns : list\n",
    "        Encode edilecek sÃ¼tunlar\n",
    "    encoding_type : str\n",
    "        'label' - Label Encoding\n",
    "        'onehot' - One-Hot Encoding\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame, list of stages\n",
    "    \"\"\"\n",
    "    stages = []\n",
    "    \n",
    "    for column in columns:\n",
    "        if column not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # String Indexer\n",
    "        indexer = StringIndexer(\n",
    "            inputCol=column,\n",
    "            outputCol=f\"{column}_index\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        stages.append(indexer)\n",
    "        \n",
    "        if encoding_type == \"onehot\":\n",
    "            # One-Hot Encoder\n",
    "            encoder = OneHotEncoder(\n",
    "                inputCols=[f\"{column}_index\"],\n",
    "                outputCols=[f\"{column}_onehot\"]\n",
    "            )\n",
    "            stages.append(encoder)\n",
    "    \n",
    "    # Pipeline oluÅŸtur ve uygula\n",
    "    if stages:\n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        model = pipeline.fit(df)\n",
    "        df_encoded = model.transform(df)\n",
    "        return df_encoded, stages\n",
    "    \n",
    "    return df, stages\n",
    "\n",
    "print(\"âœ… Kategorik encoding fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAYISAL VERÄ° NORMALÄ°ZASYONU\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_numerical_columns(df, columns, method=\"standard\"):\n",
    "    \"\"\"\n",
    "    SayÄ±sal sÃ¼tunlarÄ± normalize eder\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    columns : list\n",
    "        Normalize edilecek sÃ¼tunlar\n",
    "    method : str\n",
    "        'standard' - StandardScaler (z-score)\n",
    "        'minmax' - MinMaxScaler (0-1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    # Ã–nce VectorAssembler ile sÃ¼tunlarÄ± birleÅŸtir\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=columns,\n",
    "        outputCol=\"features_raw\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    df_assembled = assembler.transform(df)\n",
    "    \n",
    "    # Scaler seÃ§\n",
    "    if method == \"standard\":\n",
    "        scaler = StandardScaler(\n",
    "            inputCol=\"features_raw\",\n",
    "            outputCol=\"features_scaled\",\n",
    "            withStd=True,\n",
    "            withMean=True\n",
    "        )\n",
    "    else:  # minmax\n",
    "        scaler = MinMaxScaler(\n",
    "            inputCol=\"features_raw\",\n",
    "            outputCol=\"features_scaled\"\n",
    "        )\n",
    "    \n",
    "    # Scaler'Ä± uygula\n",
    "    scaler_model = scaler.fit(df_assembled)\n",
    "    df_scaled = scaler_model.transform(df_assembled)\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "print(\"âœ… Normalizasyon fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ğŸµ Ses Ã–znitelik Ã‡Ä±karma (Audio Feature Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SES Ã–ZNÄ°TELÄ°K Ã‡IKARMA FONKSÄ°YONLARI\n",
    "# =============================================================================\n",
    "\n",
    "def extract_audio_features(audio_path, sr=16000, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Ses dosyasÄ±ndan Ã¶znitelik Ã§Ä±karÄ±r\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_path : str\n",
    "        Ses dosyasÄ± yolu\n",
    "    sr : int\n",
    "        Ã–rnekleme oranÄ±\n",
    "    n_mfcc : int\n",
    "        MFCC sayÄ±sÄ±\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Ã–znitelikler\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ses dosyasÄ±nÄ± yÃ¼kle\n",
    "        y, sr = librosa.load(audio_path, sr=sr)\n",
    "        \n",
    "        # Temel Ã¶znitelikler\n",
    "        features = {}\n",
    "        \n",
    "        # 1. MFCC (Mel-Frequency Cepstral Coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        for i in range(n_mfcc):\n",
    "            features[f'mfcc_{i}_mean'] = np.mean(mfccs[i])\n",
    "            features[f'mfcc_{i}_std'] = np.std(mfccs[i])\n",
    "        \n",
    "        # 2. Chroma Features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        features['chroma_mean'] = np.mean(chroma)\n",
    "        features['chroma_std'] = np.std(chroma)\n",
    "        \n",
    "        # 3. Spectral Centroid\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
    "        features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
    "        \n",
    "        # 4. Spectral Bandwidth\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
    "        features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
    "        \n",
    "        # 5. Spectral Rolloff\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
    "        features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
    "        \n",
    "        # 6. Zero Crossing Rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features['zcr_mean'] = np.mean(zcr)\n",
    "        features['zcr_std'] = np.std(zcr)\n",
    "        \n",
    "        # 7. RMS Energy\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features['rms_mean'] = np.mean(rms)\n",
    "        features['rms_std'] = np.std(rms)\n",
    "        \n",
    "        # 8. Duration\n",
    "        features['duration'] = librosa.get_duration(y=y, sr=sr)\n",
    "        \n",
    "        # 9. Tempo\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        features['tempo'] = tempo\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Hata: {audio_path} - {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Ses Ã¶znitelik Ã§Ä±karma fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOPLU Ã–ZNÄ°TELÄ°K Ã‡IKARMA\n",
    "# =============================================================================\n",
    "\n",
    "def extract_features_batch(audio_paths, sr=16000, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Birden fazla ses dosyasÄ±ndan Ã¶znitelik Ã§Ä±karÄ±r\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_paths : list\n",
    "        Ses dosyasÄ± yollarÄ± listesi\n",
    "    sr : int\n",
    "        Ã–rnekleme oranÄ±\n",
    "    n_mfcc : int\n",
    "        MFCC sayÄ±sÄ±\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Ã–znitelik dataframe'i\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    for audio_path in tqdm(audio_paths, desc=\"Ã–znitelik Ã§Ä±karÄ±lÄ±yor\"):\n",
    "        features = extract_audio_features(audio_path, sr, n_mfcc)\n",
    "        if features is not None:\n",
    "            features['file_path'] = audio_path\n",
    "            all_features.append(features)\n",
    "    \n",
    "    df_features = pd.DataFrame(all_features)\n",
    "    return df_features\n",
    "\n",
    "print(\"âœ… Toplu Ã¶znitelik Ã§Ä±karma fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SES DOSYALARINI BUL VE Ã–ZNÄ°TELÄ°K Ã‡IKAR\n",
    "# =============================================================================\n",
    "\n",
    "# Ses dosyalarÄ±nÄ± bul\n",
    "audio_extensions = ['*.mp3', '*.wav', '*.flac', '*.ogg', '*.m4a']\n",
    "audio_files = []\n",
    "\n",
    "for ext in audio_extensions:\n",
    "    audio_files.extend(glob.glob(os.path.join(DATA_DIR, \"**\", ext), recursive=True))\n",
    "\n",
    "print(f\"ğŸ“Š Bulunan ses dosyasÄ± sayÄ±sÄ±: {len(audio_files)}\")\n",
    "\n",
    "if audio_files:\n",
    "    # Ã–rnek olarak ilk 100 dosyadan Ã¶znitelik Ã§Ä±kar (tam veri seti iÃ§in sÄ±nÄ±rÄ± kaldÄ±rÄ±n)\n",
    "    sample_size = min(100, len(audio_files))\n",
    "    print(f\"\\nğŸ”§ Ä°lk {sample_size} dosyadan Ã¶znitelik Ã§Ä±karÄ±lÄ±yor...\")\n",
    "    \n",
    "    df_audio_features = extract_features_batch(audio_files[:sample_size])\n",
    "    \n",
    "    print(f\"\\nâœ… {len(df_audio_features)} dosyadan Ã¶znitelik Ã§Ä±karÄ±ldÄ±!\")\n",
    "    print(f\"ğŸ“Š Ã–znitelik sayÄ±sÄ±: {len(df_audio_features.columns)}\")\n",
    "    \n",
    "    # Ä°lk birkaÃ§ satÄ±rÄ± gÃ¶ster\n",
    "    print(\"\\nğŸ“Š Ã–znitelik Ã–nizlemesi:\")\n",
    "    display(df_audio_features.head())\n",
    "else:\n",
    "    print(\"âš ï¸ Ses dosyasÄ± bulunamadÄ±. LÃ¼tfen DATA_DIR yolunu kontrol edin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Ã–ZNÄ°TELÄ°KLERÄ° SPARK DATAFRAME'E DÃ–NÃœÅTÃœR\n",
    "# =============================================================================\n",
    "\n",
    "if 'df_audio_features' in dir() and len(df_audio_features) > 0:\n",
    "    # Pandas DataFrame'i Spark DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "    spark_audio_features = spark.createDataFrame(df_audio_features)\n",
    "    \n",
    "    print(\"\\nğŸ“Š Spark Audio Features Schema:\")\n",
    "    spark_audio_features.printSchema()\n",
    "    \n",
    "    print(\"\\nğŸ“Š Ä°statistikler:\")\n",
    "    spark_audio_features.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ğŸ¤– Model EÄŸitimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EÄÄ°TÄ°M VERÄ°SÄ° HAZIRLA\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_training_data(df, feature_cols, label_col, test_size=0.2):\n",
    "    \"\"\"\n",
    "    EÄŸitim verisini hazÄ±rlar\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    feature_cols : list\n",
    "        Ã–znitelik sÃ¼tunlarÄ±\n",
    "    label_col : str\n",
    "        Etiket sÃ¼tunu\n",
    "    test_size : float\n",
    "        Test seti oranÄ±\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_df, test_df, pipeline\n",
    "    \"\"\"\n",
    "    stages = []\n",
    "    \n",
    "    # 1. Label Indexer\n",
    "    label_indexer = StringIndexer(\n",
    "        inputCol=label_col,\n",
    "        outputCol=\"label\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages.append(label_indexer)\n",
    "    \n",
    "    # 2. Vector Assembler\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_cols,\n",
    "        outputCol=\"features_raw\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    stages.append(assembler)\n",
    "    \n",
    "    # 3. Scaler\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features_raw\",\n",
    "        outputCol=\"features\",\n",
    "        withStd=True,\n",
    "        withMean=True\n",
    "    )\n",
    "    stages.append(scaler)\n",
    "    \n",
    "    # Pipeline oluÅŸtur\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    df_prepared = pipeline_model.transform(df)\n",
    "    \n",
    "    # Train/Test split\n",
    "    train_df, test_df = df_prepared.randomSplit([1-test_size, test_size], seed=42)\n",
    "    \n",
    "    print(f\"âœ… EÄŸitim seti: {train_df.count():,} kayÄ±t\")\n",
    "    print(f\"âœ… Test seti: {test_df.count():,} kayÄ±t\")\n",
    "    \n",
    "    return train_df, test_df, pipeline_model\n",
    "\n",
    "print(\"âœ… EÄŸitim verisi hazÄ±rlama fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL EÄÄ°TÄ°MÄ°\n",
    "# =============================================================================\n",
    "\n",
    "def train_models(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Birden fazla model eÄŸitir ve karÅŸÄ±laÅŸtÄ±rÄ±r\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_df : DataFrame\n",
    "        EÄŸitim verisi\n",
    "    test_df : DataFrame\n",
    "        Test verisi\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Model sonuÃ§larÄ±\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    # 1. Logistic Regression\n",
    "    print(\"\\nğŸ”„ Logistic Regression eÄŸitiliyor...\")\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        maxIter=100,\n",
    "        regParam=0.01\n",
    "    )\n",
    "    lr_model = lr.fit(train_df)\n",
    "    lr_predictions = lr_model.transform(test_df)\n",
    "    lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "    results['Logistic Regression'] = {'model': lr_model, 'accuracy': lr_accuracy}\n",
    "    print(f\"   âœ… Accuracy: {lr_accuracy:.4f}\")\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print(\"\\nğŸ”„ Random Forest eÄŸitiliyor...\")\n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        numTrees=100,\n",
    "        maxDepth=10,\n",
    "        seed=42\n",
    "    )\n",
    "    rf_model = rf.fit(train_df)\n",
    "    rf_predictions = rf_model.transform(test_df)\n",
    "    rf_accuracy = evaluator.evaluate(rf_predictions)\n",
    "    results['Random Forest'] = {'model': rf_model, 'accuracy': rf_accuracy}\n",
    "    print(f\"   âœ… Accuracy: {rf_accuracy:.4f}\")\n",
    "    \n",
    "    # 3. Gradient Boosted Trees\n",
    "    print(\"\\nğŸ”„ Gradient Boosted Trees eÄŸitiliyor...\")\n",
    "    # GBT sadece binary classification destekler, multi-class iÃ§in RF kullanÄ±labilir\n",
    "    # Burada label sayÄ±sÄ±nÄ± kontrol edelim\n",
    "    num_labels = train_df.select(\"label\").distinct().count()\n",
    "    \n",
    "    if num_labels == 2:\n",
    "        gbt = GBTClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            maxIter=50,\n",
    "            maxDepth=5,\n",
    "            seed=42\n",
    "        )\n",
    "        gbt_model = gbt.fit(train_df)\n",
    "        gbt_predictions = gbt_model.transform(test_df)\n",
    "        gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
    "        results['GBT'] = {'model': gbt_model, 'accuracy': gbt_accuracy}\n",
    "        print(f\"   âœ… Accuracy: {gbt_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ GBT multi-class desteklemez ({num_labels} sÄ±nÄ±f var)\")\n",
    "    \n",
    "    # 4. Multilayer Perceptron\n",
    "    print(\"\\nğŸ”„ Multilayer Perceptron eÄŸitiliyor...\")\n",
    "    # Ã–znitelik boyutunu al\n",
    "    feature_size = len(train_df.select(\"features\").first()[0])\n",
    "    \n",
    "    layers = [feature_size, 128, 64, num_labels]\n",
    "    \n",
    "    mlp = MultilayerPerceptronClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"label\",\n",
    "        layers=layers,\n",
    "        maxIter=100,\n",
    "        seed=42\n",
    "    )\n",
    "    mlp_model = mlp.fit(train_df)\n",
    "    mlp_predictions = mlp_model.transform(test_df)\n",
    "    mlp_accuracy = evaluator.evaluate(mlp_predictions)\n",
    "    results['MLP'] = {'model': mlp_model, 'accuracy': mlp_accuracy}\n",
    "    print(f\"   âœ… Accuracy: {mlp_accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Model eÄŸitim fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CROSS VALIDATION Ä°LE MODEL OPTÄ°MÄ°ZASYONU\n",
    "# =============================================================================\n",
    "\n",
    "def optimize_model_with_cv(train_df, model_type=\"rf\", num_folds=5):\n",
    "    \"\"\"\n",
    "    Cross-validation ile model optimizasyonu\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_df : DataFrame\n",
    "        EÄŸitim verisi\n",
    "    model_type : str\n",
    "        'rf' - Random Forest\n",
    "        'lr' - Logistic Regression\n",
    "    num_folds : int\n",
    "        Fold sayÄ±sÄ±\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_model, cv_model\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ {model_type.upper()} iÃ§in Cross-Validation baÅŸlÄ±yor...\")\n",
    "    \n",
    "    if model_type == \"rf\":\n",
    "        model = RandomForestClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\",\n",
    "            seed=42\n",
    "        )\n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "            .addGrid(model.numTrees, [50, 100, 200]) \\\n",
    "            .addGrid(model.maxDepth, [5, 10, 15]) \\\n",
    "            .build()\n",
    "    else:  # lr\n",
    "        model = LogisticRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label\"\n",
    "        )\n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "            .addGrid(model.regParam, [0.01, 0.1, 0.5]) \\\n",
    "            .addGrid(model.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "            .build()\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    crossval = CrossValidator(\n",
    "        estimator=model,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=num_folds,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    cv_model = crossval.fit(train_df)\n",
    "    \n",
    "    print(f\"âœ… En iyi CV skoru: {max(cv_model.avgMetrics):.4f}\")\n",
    "    \n",
    "    return cv_model.bestModel, cv_model\n",
    "\n",
    "print(\"âœ… Cross-validation fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ğŸ“ˆ Model DeÄŸerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEÄERLENDÄ°RME FONKSÄ°YONLARI\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(predictions, label_col=\"label\", prediction_col=\"prediction\"):\n",
    "    \"\"\"\n",
    "    Model performansÄ±nÄ± deÄŸerlendirir\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : DataFrame\n",
    "        Tahmin sonuÃ§larÄ±\n",
    "    label_col : str\n",
    "        GerÃ§ek etiket sÃ¼tunu\n",
    "    prediction_col : str\n",
    "        Tahmin sÃ¼tunu\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Metrikler\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Multi-class evaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=label_col,\n",
    "        predictionCol=prediction_col\n",
    "    )\n",
    "    \n",
    "    # Accuracy\n",
    "    metrics['accuracy'] = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    \n",
    "    # Precision\n",
    "    metrics['precision'] = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    \n",
    "    # Recall\n",
    "    metrics['recall'] = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    \n",
    "    # F1 Score\n",
    "    metrics['f1'] = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ“Š MODEL DEÄERLENDÄ°RME SONUÃ‡LARI\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {metrics['f1']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"âœ… DeÄŸerlendirme fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFUSION MATRIX GÃ–RSELLEÅTÄ°RME\n",
    "# =============================================================================\n",
    "\n",
    "def plot_confusion_matrix(predictions, label_col=\"label\", prediction_col=\"prediction\"):\n",
    "    \"\"\"\n",
    "    Confusion matrix gÃ¶rselleÅŸtirir\n",
    "    \"\"\"\n",
    "    # Pandas'a dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "    pred_df = predictions.select(label_col, prediction_col).toPandas()\n",
    "    \n",
    "    # Confusion matrix hesapla\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    \n",
    "    cm = confusion_matrix(pred_df[label_col], pred_df[prediction_col])\n",
    "    \n",
    "    # GÃ¶rselleÅŸtir\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('GerÃ§ek DeÄŸer')\n",
    "    plt.xlabel('Tahmin')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nğŸ“Š Classification Report:\")\n",
    "    print(classification_report(pred_df[label_col], pred_df[prediction_col]))\n",
    "\n",
    "print(\"âœ… Confusion matrix fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL SONUÃ‡LARINI KARÅILAÅTIR\n",
    "# =============================================================================\n",
    "\n",
    "def compare_models(results):\n",
    "    \"\"\"\n",
    "    Model sonuÃ§larÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±r ve gÃ¶rselleÅŸtirir\n",
    "    \"\"\"\n",
    "    model_names = list(results.keys())\n",
    "    accuracies = [results[m]['accuracy'] for m in model_names]\n",
    "    \n",
    "    # GÃ¶rselleÅŸtir\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(model_names, accuracies, color=['#3498db', '#2ecc71', '#e74c3c', '#9b59b6'])\n",
    "    \n",
    "    # DeÄŸerleri bar Ã¼zerine yaz\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{acc:.4f}', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.title('Model KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=14)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # En iyi model\n",
    "    best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
    "    print(f\"\\nğŸ† En iyi model: {best_model_name} (Accuracy: {results[best_model_name]['accuracy']:.4f})\")\n",
    "\n",
    "print(\"âœ… Model karÅŸÄ±laÅŸtÄ±rma fonksiyonu hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ğŸ’¾ Model Kaydetme ve YÃ¼kleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL KAYDETME\n",
    "# =============================================================================\n",
    "\n",
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Modeli kaydeder\n",
    "    \"\"\"\n",
    "    model.write().overwrite().save(path)\n",
    "    print(f\"âœ… Model kaydedildi: {path}\")\n",
    "\n",
    "def load_model(model_class, path):\n",
    "    \"\"\"\n",
    "    Modeli yÃ¼kler\n",
    "    \"\"\"\n",
    "    from pyspark.ml.classification import (\n",
    "        RandomForestClassificationModel,\n",
    "        LogisticRegressionModel,\n",
    "        MultilayerPerceptronClassificationModel\n",
    "    )\n",
    "    \n",
    "    model_classes = {\n",
    "        'rf': RandomForestClassificationModel,\n",
    "        'lr': LogisticRegressionModel,\n",
    "        'mlp': MultilayerPerceptronClassificationModel\n",
    "    }\n",
    "    \n",
    "    model = model_classes[model_class].load(path)\n",
    "    print(f\"âœ… Model yÃ¼klendi: {path}\")\n",
    "    return model\n",
    "\n",
    "print(\"âœ… Model kaydetme/yÃ¼kleme fonksiyonlarÄ± hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ğŸš€ Tam Pipeline Ã‡alÄ±ÅŸtÄ±rma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TAM PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def run_full_pipeline(data_dir, label_column=None):\n",
    "    \"\"\"\n",
    "    Tam pipeline Ã§alÄ±ÅŸtÄ±rÄ±r\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str\n",
    "        Veri dizini\n",
    "    label_column : str\n",
    "        Etiket sÃ¼tunu adÄ± (None ise otomatik tespit edilir)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸš€ TAM PIPELINE BAÅLATILIYOR\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Ses dosyalarÄ±nÄ± bul\n",
    "    print(\"\\nğŸ“‚ 1. Ses dosyalarÄ± aranÄ±yor...\")\n",
    "    audio_extensions = ['*.mp3', '*.wav', '*.flac', '*.ogg', '*.m4a']\n",
    "    audio_files = []\n",
    "    for ext in audio_extensions:\n",
    "        audio_files.extend(glob.glob(os.path.join(data_dir, \"**\", ext), recursive=True))\n",
    "    print(f\"   âœ… {len(audio_files)} ses dosyasÄ± bulundu\")\n",
    "    \n",
    "    if not audio_files:\n",
    "        print(\"   âš ï¸ Ses dosyasÄ± bulunamadÄ±!\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Ã–znitelik Ã§Ä±kar\n",
    "    print(\"\\nğŸ”§ 2. Ã–znitelikler Ã§Ä±karÄ±lÄ±yor...\")\n",
    "    df_features = extract_features_batch(audio_files)\n",
    "    print(f\"   âœ… {len(df_features)} dosyadan Ã¶znitelik Ã§Ä±karÄ±ldÄ±\")\n",
    "    \n",
    "    # 3. Spark DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "    print(\"\\nâš¡ 3. Spark DataFrame oluÅŸturuluyor...\")\n",
    "    spark_df = spark.createDataFrame(df_features)\n",
    "    print(f\"   âœ… DataFrame oluÅŸturuldu\")\n",
    "    \n",
    "    # 4. EÄŸer etiket varsa model eÄŸit\n",
    "    if label_column and label_column in spark_df.columns:\n",
    "        print(f\"\\nğŸ¤– 4. Model eÄŸitimi baÅŸlÄ±yor (Etiket: {label_column})...\")\n",
    "        \n",
    "        # Ã–znitelik sÃ¼tunlarÄ±nÄ± al (etiket ve file_path hariÃ§)\n",
    "        feature_cols = [c for c in spark_df.columns if c not in [label_column, 'file_path']]\n",
    "        \n",
    "        # EÄŸitim verisini hazÄ±rla\n",
    "        train_df, test_df, prep_pipeline = prepare_training_data(\n",
    "            spark_df, feature_cols, label_column\n",
    "        )\n",
    "        \n",
    "        # Model eÄŸit\n",
    "        results = train_models(train_df, test_df)\n",
    "        \n",
    "        # KarÅŸÄ±laÅŸtÄ±r\n",
    "        compare_models(results)\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ Etiket sÃ¼tunu bulunamadÄ±. Sadece Ã¶znitelik Ã§Ä±karma yapÄ±ldÄ±.\")\n",
    "        return spark_df\n",
    "\n",
    "print(\"âœ… Tam pipeline fonksiyonu hazÄ±r!\")\n",
    "print(\"\\nğŸ’¡ KullanÄ±m: results = run_full_pipeline(DATA_DIR, label_column='gender')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE'I Ã‡ALIÅTIR\n",
    "# =============================================================================\n",
    "\n",
    "# Etiket sÃ¼tunu varsa belirtin (Ã¶rn: 'gender', 'age', 'accent' vb.)\n",
    "# Common Voice veri setinde genellikle bu etiketler bulunur\n",
    "\n",
    "# Ã–rnek kullanÄ±m:\n",
    "# results = run_full_pipeline(DATA_DIR, label_column='gender')\n",
    "\n",
    "# Etiket olmadan sadece Ã¶znitelik Ã§Ä±karma:\n",
    "# df_features = run_full_pipeline(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ğŸ§¹ Temizlik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPARK SESSION'I KAPAT\n",
    "# =============================================================================\n",
    "\n",
    "# Ä°ÅŸlem bittiÄŸinde Spark session'Ä± kapat\n",
    "# spark.stop()\n",
    "# print(\"âœ… Spark session kapatÄ±ldÄ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Notlar\n",
    "\n",
    "### Veri Seti HakkÄ±nda\n",
    "- Mozilla Common Voice, Ã§eÅŸitli dillerde ses kayÄ±tlarÄ± iÃ§eren aÃ§Ä±k kaynaklÄ± bir veri setidir\n",
    "- Her kayÄ±t iÃ§in metadata (cinsiyet, yaÅŸ, aksan vb.) bulunabilir\n",
    "\n",
    "### Preprocessing AdÄ±mlarÄ±\n",
    "1. **Veri Ä°ndirme**: Kaggle API ile\n",
    "2. **Ses Ã–znitelik Ã‡Ä±karma**: MFCC, Chroma, Spectral Ã¶znitelikleri\n",
    "3. **Veri Temizleme**: Null deÄŸerler, normalizasyon\n",
    "4. **Model EÄŸitimi**: Logistic Regression, Random Forest, MLP\n",
    "\n",
    "### Ä°puÃ§larÄ±\n",
    "- BÃ¼yÃ¼k veri setleri iÃ§in `sample_size` parametresini artÄ±rÄ±n\n",
    "- Bellek sorunlarÄ± iÃ§in Spark config ayarlarÄ±nÄ± dÃ¼zenleyin\n",
    "- Google Colab'da Ã§alÄ±ÅŸÄ±rken disk alanÄ±na dikkat edin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
